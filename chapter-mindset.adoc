= The Test Automation Mindset

In this chapter we'll explore the mindset that we apply when thinking about what kinds of automated tests we want to implement.

== Kinds of Automated Tests

_Unit-_, _Component-_, _Integration-_, _System-_, _End-to-End-_, _Acceptance-_, _Behaviour-_, _Property-_, _Contract-_, _Snapshot-_, _Performance-_, _Security-_ and _Architecture-Tests_ as well as things like _Static Code Analysis_.

These are a lot of labels for seemingly complex things, that actually come down to just a handful of properties:

* *scope* - _"Where does the test start and where does it end?"_
** pure function
** class method
** class
** package
** module
** deployment unit
** repository
** system

* *aspect under test*
** functionality - _"Does my code do what I want it to?"_
** integration - _"Does my code integrate with other code, framework features, technologies etc. as I intended it to?"_
** rules - _"Does my code follow our defined rules? (e.g. architecture, security, style)"_
** performance - _"Does my code execute as fast as I need it to?"_

The goal of efficient and sustainable automated testing is to keep the following **costs** down, while still covering all relevant risks:

* *implementation effort* - _"How long does it take me to initially implement the test?"_
* *maintenance effort* - _"How often, and how much time, do I need to invest in maintaining the test?"_
* *execution time* - _"How much time is needed to execute the test?"_

By far the most important of these, for projects with long life spans, is _execution time_.
Test suites will be run thousands of times over the life span of a project.
Wasted minutes will accumulate to a lot of wasted time (= money) very fast.

A good test suite should be executable by developers anytime they need to make sure everything still works.
Without having to schedule a coffee break or meeting to kill time while waiting for the result.

== On Labeling Tests

Humans tend to put things into mental boxes in order to make sense of them.
This also happens when we think about tests.
As can be seen by all the different kinds of tests in the previous chapter.
There is just one problem:

_Things also tend to be more complicated and multi-faceted to just neatly fit into a box._

As an example, despite all attempts to standardise terms like _Integration-Test_ whenever you start to work with new people, you'll soon learn that they have a more or less different understanding of those terms than you.
Like spaces over tabs and other programming culture wars, this might never be resolved.

The good news is that labels are effectively worthless.
It does not matter what you call your tests.
Whatever they do, they should do it efficiently and mitigate some relevant risk.
The only reason to categorize your tests should be, to better communicate with your team when talking about them.
But this should be done with the conscious knowledge that whatever labels you define, might only be valid in your team's context.

Instead of wasting time arguing about how to name certain kinds of similar tests, invest that time into building common understanding about how your team wants to approach testing certain similar components.
(e.g. how to test repositories, controllers, cached method calls, etc.)

== Examples

*Quick disclaimer:* As most things are identical between front and backend, the following examples rarely go into detail on which part of your application it applies to.

=== I want to make sure that the result of my pure function, given specific inputs, is correct.

* **scope:**
_function_

* **aspect under test:**
_functionality_

* **implementation effort:**
_Low_.
There are no dependencies to handle.
Permutation over possible input combinations is easy with most modern test automation frameworks.

* **maintenance effort:**
_Very low_.
Pure functions usually don't change a lot over time.
They are also self-contained and can therefore be tested as blackboxes.

* **execution time:**
_extremely fast_

The common label for this type of test is _Unit-Test_.
Which in this case should be rather uncontroversial.

=== I want to make sure that my repository class' SQL statements are syntactically correct and do what I expect them to.

* **scope:**
_class_

* **aspects under test:**
_functionality_ ("does what I want it to") & _integration_ ("SQL statements are syntactically correct")

* **implementation effort:**
Depending on the applied level of abstraction and the complexity of the underlying database, it varies between _low_ and _medium_.
Most of which usually comes from managing test data and therefore the complexity of the underlying database schema.

* **maintenance effort:**
Usually _low_, but bad abstractions can lead to unnecessary overhead - keep it simple!

* **execution time:**
Individual tests will be _very fast_.
The cost of the initial test setup depends on whether an in-memory database is a suitable replacement for the real thing.
If it is not, bootstrapping will take a couple of extra seconds (e.g. using Docker containers).

In regard to labeling your tests, this is a rather interesting example.
Some might label this type of test either as a _Unit-Test_ or an _Integration-Test_.

There are arguments for both sides.
If you think of _Integration-Test_ on the System-Component or Deployment-Unit level, then this is a _Unit-Test_.
If you think of _Integration-Test_ as anything that tests the integration of your code with anything else, then this is an _Integration-Test_.

In order to make things more clear, a more precise label would be _Technology Integration-Test_.
This specifies that its goal is to test that we are using a technology, in this case a database, correctly.

But as mentioned above, don't waste too much time labeling tests in the first place.

=== I want to make sure that the component responsible for talking to an HTTP API of another service runtime handles all relevant scenarios as intended.

* **scope:**
_class_ or _package_ (might include some function calls and helper classes as well as the main component)

* **aspects under test:**
_functionality_ ("handles all relevant scenarios") & _integration_ ("talking to an HTTP API")

* **implementation effort:**
Depending on the complexity of the API, it varies between _low_ and _medium_.
Most of which usually comes from managing the expected responses for all relevant scenarios.

* **maintenance effort:**
_Low_.
If the API is stable.
Otherwise, you'll have to change one thing or another every time the API changes.

* **execution time:**
Individual tests will be _very fast_.
Bootstrapping a service simulator will add about _another 1 to 2 seconds_ to the overall cost.

The goal is to test that the code is sending valid HTTP requests with the expected content, as well as that expected responses are handled correctly.
Writing a _Unit-Test_ and using mocks (e.g. for the HTTP client) will not actually test anything other than that the code is invoked as you've written it.
The most important aspect under test here is that the actually produced HTTP requests look as expected and that different responses are parsed and handled correctly.

None of which is possible without using an external simulator.

=== I want to make sure my button triggers my logic service when pressed.

* **scope:**
_component_

* **aspects under test:**
_functionality_ ("button triggers my logic service")

* **implementation effort:**
Generally _low_ since most common testing frameworks offer enough DOM abstaction to perform the action and the service logic can be stubbed or mocked.

* **maintenance effort:**
Usually _low_. But can be problematic due to a non optimal access strategy such as via CSS classes.

* **execution time:**
_Very fast_ with modern testing frameworks that do not rely on an actual browser.

The goal is to ensure that your logic is acutually triggered by a user clicking the button, not to validate that the logic is working correctly. That can be done in a separate test for that particular logic. For this kind of test, we can safely mock the service logic.

A common label for this type of test would be _Unit-Test_. Dependent on the Framework, we could use things as Angulars TestBed, where one could argue for labeling it as an _Integration-Test_ but that's splitting hairs.

=== I want to make sure that my basic ui component is rendered in a consistent way.

* **scope:**
_component_ or _module_

* **aspects under test:**
_rendering_ ("rendered in a consistent way")

* **implementation effort:**
Given a good testing framework and sufficient architecture _almost for free_.

* **maintenance effort:**
_Almost none_. Shallow rendering (or as shallow as possible) ensures small snapshots and changes can be processed quickly.

* **execution time:**
_Very fast_ with modern testing frameworks that do not rely on an actual browser.

The goal of those tests is to ensure that your rendering only changes when it is supposed to do so. Therefore, good separation of concerns is needed to ensure, that you do not render too much information in a given test. Shallow rendering helps a lot with that as we strip out any unwanted and bloated components, that do not actually benefit the intend of the test.

A common label for this type of test would be _Unit-Test_ or _Snapshot-Test_. Tests like those fall under the topic of _Approval Testing_.

=== I want to ensure that my complex rendering logic is visually stable.

* **scope:**
_deployment unit_

* **aspects under test:**
_rendering_ ("complex rendering logic")

* **implementation effort:**
_Low to medium_ in most circumstances, depending on the UI complexity. Increases proportional to the data requirements for the rendering.

* **maintenance effort:**
_Medium to high_ as those tests will rely on actual browser rendering which can change for several reasons those requiring regular attention.

* **execution time:**
_Medium to slow_ as we do require both a browser and some for of image comparison.

We want to focus on the complex rendering logic and thus isolate the important parts, that can not be appropriately checked with faster and easier to implement testing approaches. Most often we are testing actual rendering, e.g. for integrations against canvas libraries. As those tests often rely on screenshots being taken, it is important to isolate the important parts as the tests become very brittle otherwise. For example, one could only validate the appearance of a certain canvas, instead of the complete page.

A common label for this type of test would be _End-to-End-Test_ or _Visual-Regression-Test_. Tests like those fall under the topic of _Approval Testing_ but with images instead of text as a base line.

=== I want to make sure my state management handles changes appropriately.

* **scope:**
_module_

* **aspects under test:**
_functionality_ ("handles changes") & _integration_ ("state management")

* **implementation effort:**
_Low_ as the integration part is provided by testing utility of good libraries.

* **maintenance effort:**
Given good isolation of the state management, _low_.

* **execution time:**
_Very fast_ as there is no need for any browser interaction.

In order to ensure good testability, it is advisable to encapsulate state management behind some form of facade to test the state module in isolation and mock its integration in other components.
As we want to ensure that our state management is handled correctly, only mock side effects that are triggered but no other pieces like state manipulation functions.

A common label for this type of test would be _Unit-Test_ or _Integration-Test_. But as the integration is provided with mock libraries for popular state management solutions, most would argue for _Unit-Test_.

=== I want to make sure my callbacks for asynchronous timers doing the right thing at the appropriate time.

* **scope:**
_function_ or _class_

* **aspects under test:**
_functionality_ ("callbacks doing the right thing" and "at the appropriate time")

* **implementation effort:**
_Low to medium_ as most popular testing frameworks offer some form of timer manipulation but the timer complexity is often complex.

* **maintenance effort:**
_Low_ as the majority of test code will only handle (hopefully) pure functions.

* **execution time:**
_Very fast_ as there is no need for any browser interaction and timers are simulated.

The goal is not to wait for the timers to pass but to make sure the timers are triggered when they need to be triggered.
This is done by simulating time progression to a certain point and check whether the logic is called.

A common label for this type of test would be _Unit-Test_.

=== I want to make sure that an orchestrating service class behaves like it should even when exceptions occur.

* **scope:**
_class_

* **aspects under test:**
_functionality_ ("behaves like it should")

* **implementation effort:**
Depending on the complexity of the process being orchestrated (e.g. number of other components involved), it will vary between _low_ and _medium_.

* **maintenance effort:**
_Usually low_ when tests are implemented efficiently.
Higher if tests were written too close to the production code (white-box tests).

* **execution time:**
_extremely fast_

The goal is to verify behavioral aspects of the class under test.
This is done by initializing an instance of the class with most, if not all, dependencies mocked.
Dependencies include references to local resources (e.g. system clock) and other classes (e.g. event handler, repositories etc.).
Pure functions and other static calls should _not be mocked_!

If your static code needs mocking because of some kind of state, then that's bad static code!

Tests usually involve checking that the correct parts of the input data are given to the dependencies and that their results are in turn used correctly in the following steps.
If there are side effects (e.g. publishing of events) in the component's code, their invocation is also tested.

=== I want to make sure that security rules, like the way a user is authenticated, for certain paths of my HTTP-based API are enforced.

* **scope:**
_deployment unit_

* **aspects under test:**
_functionality_ ("works as intended"), _rules_ ("security rules") & _integration_ ("user is authenticated", "HTTP-based API")

* **implementation effort:**
If security is implemented in a test-friendly way, it _can be very low_.
If not it _might be much higher_.

* **maintenance effort:**
_Low_.
Once established, security rules do not change very often.

* **execution time:**
Individual tests will be _very fast_.
Bootstrapping the _deployment unit_ to start with the minimum set of components, to make the test meaningful might take a couple of seconds.

The goal is to test that certain security rules are applied for parts of an API using a certain authentication technology.
The security framework, the authentication protocols and HTTP as a transport layer are all technologies being integrated with your own code.

=== I want to make sure two of my service runtimes can talk to each other over HTTP and messages.

* **scope:**
_deployment units_

* **aspects under test:**
_functionality_ ("can talk") & _integration_ ("two of my service runtimes", "HTTP and messages")

* **implementation effort:**
_High_ if tests are implemented in a way that needs both service runtimes to be involved at the same time.
_Much lower_ if something like contracts (e.g. PACT, Spring Cloud Contract etc.) are used to decouple both services from each other.

* **maintenance effort:**
Every time something is changed by either of the deployment units, these tests need to be changed as well.
Depending on how the tests are set up (full integration vs. contract-based), the actual effort for each change might be _very low_ or _very high_.

* **execution time:**
Depends heavily on how the tests are set up.
Bootstrapping two deployment units and having to also set up test data for the target _can take a lot of time_.
Running against a simulation (e.g. contract-based integration) on the other hand is _very fast_.

This is an excellent example how knowing the scope and different aspects of what you want to test, and choosing the right tools to do so efficiently, makes the difference between tests running for minutes or just a few seconds.

Practices like having contracts for testing the integration of separate deployment units might just mitigate 90% of the risk.
But they are much more efficient than full integration tests which might mitigate 92% of the risk

=== I want to make sure that a specific part of my user journey works with front- and backend involved.

* **scope:**
_deployment unit(s)_

* **aspects under test:**
_functionality_ ("user journey works") & _integration_ ("with front- and backend involved")

* **implementation effort:**
High if tests are implemented in a way that needs both runtimes and a browser to be involved at the same time.
Much lower if communication is ensured via contract testing (e.g. PACT) and the user journey test runs against mocks.

* **maintenance effort:**
Every time something is changed by either of the deployment units, these tests need to be changed as well.
Depending on how the tests are set up (full integration vs. contract-based), the actual effort for each change might be very low or very high.

* **execution time:**
Depends on how the tests are set up.
Bootstrapping two deployment units and having to also set up test data for the target can take a lot of time.
Running against a simulation on the other hand is very fast.

The goal of this test is not to test that your database in the backend saves your data as expected but to test that your user journey works. Decoupling different deployment units (e.g. via contract-based tests), helps with that and enables simulating any backend calls as the communication is tested on other levels. This will decrease both implementation and maintenance complexity.

=== I want to make sure, that certain architectural principals are followed in our codebase.

* **scope:**
_modules_ / _deployment unit_ / _repository_

* **aspects under test:**
following of (architectural) rules

* **implementation effort:**
_Medium_.
Tools like ArchUnit for the JVM make defining architectural rules and checking them as part of the regular tests easy.
Specifying more complex rules might take a while though.

* **maintenance effort:**
_Low_.
Architecture, once established, does not tend to change a lot over time.

* **execution time:**
Depends on the size of the code base.
Usually just a _couple of seconds_.
With the initial startup & analysis taking up most of the time.
